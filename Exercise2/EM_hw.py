import numpy as np
import numpy.random as npr
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
import os

# set the working directory
os.chdir("C:/Users/pistl/Desktop/hw2/")

# print the current working directory
os.getcwd()

# read in data 
def sample_GMM():    
    data = np.loadtxt(fname = "C:/Users/pistl/Desktop/hw2/dataSets/gmm.txt",usecols=(0, 1), skiprows=1)
    #print(data)
    return data

# Initialize parameters uniformly
def initialize_uniformly(data,D,K):
    
    mus = npr.uniform(low=0.7, high=1.3, size = [K, D])
    sigmas = np.zeros((K,D,D))
    for k in range(K):
        sigmas[k,:,:] = np.eye(D,D)
    pis = npr.dirichlet(np.ones(K))  #mixture coefficients
    
    return mus,sigmas,pis


# E-step of EM algorithm
def e_step(data,D,K,mus,sigmas,pis):
    
    # calculate responsibilities (= probability that data point was generated by cluster k)
    sums = np.zeros(N)
    for n in range(N):
        for k in range(K): 
                sums[n] += pis[k] * multivariate_normal.pdf(data[n],mus[k],sigmas[k])
    resp = np.zeros((N,K))
    for n in range(N):
        for k in range(K):
                resp[n,k] = pis[k] * multivariate_normal.pdf(data[n],mus[k],sigmas[k]) / sums[n]
    return resp


# M-step of EM algorithm
def m_step(data,D,K,resp):
    
    # calculate N_k
    N_k = np.zeros(K)
    N_k = np.sum(resp,axis=0)
    N_sum = np.sum(N_k)
    
    # update parameters
    # update mu
    mu = np.zeros((K,D))
    for k in range(K):
        for n in range(N):
            mu[k] += resp[n,k] * data[n,:]
        mu[k] = mu[k] / N_k[k]
        
    # update sigma
    sigma = np.zeros((K,D,D))
    for k in range(K):
        for n in range(N):
            sigma[k,:,:] += resp[n,k] * np.matmul(np.atleast_2d(data[n,:] - mu[k,:]).T, np.atleast_2d(data[n,:] - mu[k,:]))
        sigma[k,:,:] = sigma[k,:,:] / N_k[k]
    
    # update pi
    pi = np.zeros(K)
    for k in range(K):
        pi[k] = N_k[k] / N_sum
    
    return mu, sigma, pi


# calculate loglikelihood of the data under the parameter estimation
def loglike(data,D,K,pis,mus,sigmas):
    
    #calculate loglikelihood
    llh = [0]
    for n in range(N):
        for k in range(K):
            llh += np.log(pis[k] * multivariate_normal.pdf(data[n],mus[k],sigmas[k]))
        
    return llh
    
          
# EM main  loop: sample data from GMM, iterate e- and m-steps 
def em_main():           
    
    data = sample_GMM()
    K = 4
    D = 2
    N = 500
    x = data[:,0]
    y = data[:,1]
    
    # initialize parameters
    init = initialize_uniformly(data,D,K)
    mus = init[0]
    sigmas = init[1]
    pis = init[2]
    
    # iterate e-step and m-step
    iter = 0
    #llh_new, llh_old = 0, 1e-8
    while iter < 30:
        iter +=1
        
        ### calculate loglikelihood
        #llh_old= loglike(data,D,K,pis,mus,sigmas)
        
        ### execute e_step
        resp = e_step(data,D,K,mus,sigmas,pis)
        
        ### execute m_step
        m = m_step(data,D,K,resp) 
        mus = m[0]
        sigmas = m[1]
        pis = m[2]
        
        ### calculate loglikelihood
        #llh_new = loglike(data,D,K,pis,mus,sigmas)
    
    plot_data = plt.scatter(x,y,c='black', s=2)

    # Generate grid points
    dimension = 100
    x, y = np.meshgrid(np.linspace(-2,4,dimension),np.linspace(-2,5,dimension))
    xy = np.column_stack([x.flat, y.flat])
    
    # density values at the grid points
    Z_1 = multivariate_normal.pdf(xy, mus[0,:], sigmas[0,:]).reshape(x.shape)
    Z_2 = multivariate_normal.pdf(xy, mus[1,:], sigmas[1,:]).reshape(x.shape)
    Z_3 = multivariate_normal.pdf(xy, mus[2,:], sigmas[2,:]).reshape(x.shape)
    Z_4 = multivariate_normal.pdf(xy, mus[3,:], sigmas[3,:]).reshape(x.shape)
    
    # create and save plot
    fig = plt.contour(x, y, Z_1)
    fig = plt.contour(x, y, Z_2)
    fig = plt.contour(x, y, Z_3)
    fig = plt.contour(x, y, Z_4)
    plt.savefig('em_iter_30.png', dpi=300)
    
    return mus, sigmas
    
em_main()

    
               


