\newif\ifvimbug
\vimbugfalse

\ifvimbug
\begin{document}
\fi

\exercise{Bayesian Decision Theory}
In this exercise, we consider data generated by a mixture of two Gaussian distributions with parameters $\{\mu_1$, $\sigma_1\}$ and $\{\mu_2$, $\sigma_2\}$. Each Gaussian represents a class labeled $C_1$ and $C_2$, respectively. 

\begin{questions}

%----------------------------------------------

\begin{question}{Optimal Boundary}{4}
Explain in one short sentence what Bayesian Decision Theory is. What is its goal? 
What condition does hold at the optimal decision boundary? When do we decide for class $C_1$ over $C_2$?

\begin{answer}
In Bayesian decision theory a decsion is made using the degree of belief in an outcome. 

It's goal is it to minimze the risk presented in a loss funciton.

At the decision boundary $p(C_1|x) = p(C_2|x)$ holds true.

We decide for $C_1$ over $C_2$ if $P(C_1|x) > p(C_2|y)$

\end{answer}

\end{question}


%----------------------------------------------

\begin{question}{Decision Boundaries}{8}
If both classes have equal prior probabilities $p(C_1) = p(C_2)$ and the same variance $\sigma_1 = \sigma_2$, derive the decision boundary $x^*$ analytically as a function of the two means $\mu_1$ and $\mu_2$.

\begin{answer}
Since the prior are equal following equations holds true we decide for $C_1$ if:
\begin{equation}
    \frac{p(x|C_1)}{p(x|C_2)} > 1
\end{equation}


At the point of the decision boundary we can say:
\begin{equation} \label{boundary}
	p(x|C_1) = p(x|C_2)
\end{equation}

Both datasets are gausian distributed.
\begin{equation}\label{gaus1}
	\mathcal{N}(x|\mu_1, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{1}{2\sigma^2}(x-\mu_1)^2)
\end{equation}
\begin{equation}\label{gaus2}
\mathcal{N}(x|\mu_2, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{1}{2\sigma^2}(x-\mu_2)^2)
\end{equation}

Using equation \ref{gaus1} and \ref{gaus2} in equation \ref{boundary} we get:
\begin{equation}
	\frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{1}{2\sigma^2}(x-\mu_1)^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{1}{2\sigma^2}(x-\mu_2)^2)
\end{equation}

\begin{equation}
	(x-\mu_1)^2 = (x-\mu_2)^2
\end{equation}
\begin{equation}
	x = \frac{\mu_1 + \mu_2}{2}
\end{equation}

\end{answer}

\end{question}

%----------------------------------------------

\begin{question}{Different Misclassification Costs}{8}
Assume $\mu_1 > 0$, $\mu_1 = 2\mu_2$, $\sigma_1=\sigma_2$ and $p(C_1) = p(C_2)$. If misclassifying sample $x \in C_2$ as class $C_1$ is three times more expensive than the opposite, how does the decision boundary change? Derive the boundary analytically.
(There is no cost for correctly classifying samples.)

\begin{answer}
We want to minimize following risk:
\begin{equation}
	\frac{p(x|C_1)}{p(x|C_2)} > \frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}} \cdot \frac{p(C_2)}{p(C_1)}
\end{equation}
With 
\begin{equation} \label{risk}
	\lambda_{ij} = \lambda(\alpha_i|\alpha_j)
\end{equation}
The main diagonal of $\lambda$ so $\lambda_{ii}$ is the cost for correctly classifying. In our case this is zero, so $\lambda_{11} = \lambda_{22} = 0$.
Now its stated that missclassifying sample $x\in C_2$ as class $C_1$ is three times more expensive than the opposite, meaning that $\lambda_{12} = 3\cdot\lambda_{21}$. We can now use this information for equation \ref{risk}.
\begin{equation}
	\frac{p(x|C_1)}{p(x|C_2)} > \frac{3\lambda_{21}}{\lambda_{21}}\cdot \frac{p(C_2)}{p(C_1)} = 3
\end{equation} 

Now we can use the equations for the gausian distibution \ref{gaus1} and \ref{gaus2} and get:
\begin{equation}
	\frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{1}{2\sigma^2}(x-\mu_1)^2) = 3\cdot \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{1}{2\sigma^2}(x-\mu_2)^2)
\end{equation}
\begin{equation}
 \exp(-\frac{1}{2\sigma^2}(x-\mu_1)^2) = 3\cdot  \exp(-\frac{1}{2\sigma^2}(x-\mu_2)^2)
\end{equation}
Using the log to get rid of the exponential function.
\begin{equation}
	 -\frac{1}{2\sigma^2}(x-\mu_1)^2 = \ln(3)  -\frac{1}{2\sigma^2}(x-\mu_2)^2
\end{equation}

We now use $\mu_1 = 2 \mu_2$ and continue.
\begin{equation}
	-\frac{1}{2\sigma^2}(x^2-4x\mu_2 + 4\mu_2^2) + \frac{1}{2\sigma^2}(x^2-2x\mu_2+\mu_2^2) = ln(3)
\end{equation}
\begin{equation}
	-\frac{1}{2\sigma^2}(x^2 -4x\mu_2+4\mu_2^2) + \frac{1}{2\sigma^2}(-x^2+2x\mu_2-\mu_2^2) = ln(3)
\end{equation}
Transforming the terms inside the brackets brings us to
\begin{equation}
	-2x\mu_2 + 3\mu_2^2 = ln(3)\cdot(-2\sigma^2)
\end{equation}
\begin{equation}
	-2x\mu_2 = -2ln(3)\sigma^2 - 3\mu_2^2
\end{equation}
After transposing this equation we get:
\begin{equation}
	x = \frac{3\mu_2^2 + 2\ln(3)\cdot\sigma^2}{2\mu_2}
\end{equation}

\end{answer}

\end{question}

%----------------------------------------------

\end{questions}
