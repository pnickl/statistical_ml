\newif\ifvimbug
\vimbugfalse

\ifvimbug
\begin{document}
\fi

\exercise{Linear Algebra Refresher}
 

\begin{questions}

%----------------------------------------------

\begin{question}{Matrix Properties}{5}
A colleague of yours suggests matrix addition and multiplication are similar to scalars, thus commutative, distributive and associative properties can be applied.
Prove if matrix addition and multiplication are commutative and associative analytically or give counterexamples. 
Is matrix multiplication distributive with respect to matrix addition? 
Again, prove it analytically or give a counterexample.
Considering three matrices $ A, B, C$ of size $n\times n$.

\begin{answer}
To answer the questions examples calculated by following matrices will be used:


\[
A=
\begin{bmatrix}
2 & -1  \\
0 &  1 
\end{bmatrix}\quad
B=
\begin{bmatrix}
1 & 0  \\
4 & 3 
\end{bmatrix}\quad
C=
\begin{bmatrix}
2 & 2  \\
2 & 2 
.
\end{bmatrix}
\]

	
The commutative property for matrix addition states: $A+B = B + A$.
\begin{equation}
	A + B = ( \begin{array}{c c} 
		3 & -1 \\
		4 & 4 \end{array} )
\end{equation}
\begin{equation}
B + A = ( \begin{array}{c c} 
3 & -1 \\
4 & 4 \end{array} ) = A + B
\end{equation}

The commutative property for matrix multiplacation states: $A \cdot B = B \cdot A$

\begin{equation}
	A \cdot B = \begin{bmatrix}
	-2 & -3 \\
	4 & 3\\
	\end{bmatrix}
\end{equation}

\begin{equation}
B \cdot A = \begin{bmatrix}
2 & -1 \\
8 & -1\\
\end{bmatrix} = A\cdot B
\end{equation}

Thus $A \cdot B \neq B \cdot A$

The distributiv property for matrices states: $A\cdot B + A\cdot C = A (B+C)$ 


\[
\begin{bmatrix}
-2 & -3  \\
04 &  3 
\end{bmatrix}\quad
+
\begin{bmatrix}
2 & 2  \\
2 & 2 
\end{bmatrix}\quad
=
\begin{bmatrix}
2 & -1  \\
0 & 1 
\end{bmatrix}
\cdot
\begin{bmatrix}
3 & 2  \\
6 & 5
\end{bmatrix}
\]

\[
\begin{bmatrix}
0 & -1  \\
6 & 5 
\end{bmatrix}\quad
=
\begin{bmatrix}
0 & -1  \\
6 & 5
\end{bmatrix}
\]


\end{answer}

\end{question}

%----------------------------------------------

\begin{question}{Matrix Inversion}{6}
Given the following matrix 
\begin{equation*}
     A = ( \begin{array}{c c c} 
     1 & 2 & 3 \\
     1 & 2 & 4 \\
     1 & 4 & 5 \end{array} )
\end{equation*}
analytically compute its inverse $ A^{-1}$ and illustrate the steps.

If we change the matrix in
\begin{equation*}
     A = ( \begin{array}{c c c} 
     1 & 2 & 3 \\
     1 & 2 & 4 \\
     1 & 2 & 5 \end{array} )
\end{equation*}
is it still invertible? Why?

\begin{answer}
	
	First the determinant of the matrix is calculated using the Rule of Sarrus
\begin{equation}
	det(a) = 10 + 8 +12 -16 - 10 - 6 = -2 \neq 0
\end{equation}

After that we can calculate the Hauptminors of the matrix
\begin{equation}
det(A_{1,1}) = -6
\end{equation}
\begin{equation}
det(A_{1,2}) = -1
\end{equation}
\begin{equation}
det(A_{1,3}) = 2
\end{equation}
\begin{equation}
det(A_{2,1}) = -2
\end{equation}
\begin{equation}
det(A_{2,2}) = 2
\end{equation}
\begin{equation}
det(A_{2,3}) = 2
\end{equation}
\begin{equation}
det(A_{3,1}) = 2
\end{equation}
\begin{equation}
det(A_{3,2}) = 1
\end{equation}
\begin{equation}
det(A_{3,3}) = 0
\end{equation}

Using the Rule of Cramer

\begin{equation}
	x_i = \frac{det(A_i)}{det(A)} \forall i
\end{equation}

We calculate: 

\begin{equation}
A^{-1} = 
\begin{bmatrix}
	3 & -1 & -1  \\
	1/2 & -1 & 1/2 \\
	-1 & 1 & 0
\end{bmatrix}
\end{equation}

If we now change the matrix, than it's not invertible anymore, because
\begin{equation}
	det(A) = 10 + 8 + 6 - 8- 10 - 6 = 24 - 24 = 0 
\end{equation}


\end{answer}

\end{question}
	
%----------------------------------------------

\begin{question}{Matrix Pseudoinverse}{3}
	Write the definition of the right and left Moore-Penrose pseudoinverse of a generic matrix $A \in \R^{n\times m}$.
	
	Given $A \in \R^{2 \times 3}$, which one does exist? Write down the equation for computing it, specifying the dimensionality of the matrices in the intermediate steps.
	
\begin{answer}

Definition of the left Pseudoinverse:

\begin{equation}
	J^{\text{*}}J = (J^T J)^-1 J^T \cdot J = I_m
\end{equation}

Definition of the right Pseudoinverse:

\begin{equation}
J J^{\text{*}} = J \cdot  J^T (J J^T)^-1 \cdot  = I_m
\end{equation}

Given: $A \in \Re^{2x3}$ with $m > n$ which implies full row rank we use the right Pseudoinverse

\begin{equation}
	A^{\text{*}} = \underbrace{ \underbrace{A^T}_\text{3x2} \cdot \underbrace{(J J^T)^{-1}}_\text{2x2} }_\text{3x2}
\end{equation}

\begin{equation}
	\implies A^{\text{*}} \in \Re^{3x2}
\end{equation}

\end{answer}
\end{question}

%----------------------------------------------

\begin{question}{Eigenvectors \& Eigenvalues}{6}
What are eigenvectors and eigenvalues of a matrix $A$? Briefly explain why they are important in Machine Learning.

\begin{answer}
In general for Eigenvektors the following equation is true:
\begin{equation}
	A\cdot v = \lambda \cdot v
\end{equation}

Example for calculating the Eigenvalues. We start be calculating the characistic Polynom of the matrix.

\begin{equation}
	det(A-E\cdot \lambda = 0)
\end{equation}

\begin{equation}
	det
	\begin{bmatrix}
	1-\lambda & 2 & 3 \\
	1 & 2-\lambda 4 \\
	1 & 4 & 5-\lambda
	\end{bmatrix}
	= -\lambda^3 + 8 \lambda^2 + 4\lambda - 2
\end{equation}

This equation can now be solved to get the Eigenvalues. If the Eigenvalues are now placed back into the Matrix it is possible to calculate the corresponding Eigenvectors.

Eigenvectors and values are Vectors and Scalars for which 
\begin{equation}
	W\cdot \vec{v} = \lambda \vec{v}
\end{equation}
holds true.

They're defined individually for each Transformation Matrix $W$. 
If we now want to change the length of an Eigenvector we don't need to multiply it with $W$, instead we just multiply it with a corresponding Eigenvalue. Since this is a scalar instead of a Matrix, it saves computational power fo this calculation. Actually any transformation $W$ applied to an vector can be seen as a lineare combination of eigenvectors.

\begin{equation}
	\vec{u} = W \vec{v} = c_1 \lambda_1 \cdot \vec{v}_1 + ... + c_n \lambda_n \cdot \vec{v}_n
\end{equation}

This again is a huge time saver.

Eigenvalues also tell us about the numerical stability of a Matrix transformation. To achieve a vanishing matrix Eigenvalues of $\leq 1$are needed. If the Eigenvalues are bigger than one, than the matrix will explode. Ideally the Eigenvalues are all equal to one. The Markov Matrix is one of these matrices and thus used in machine learning. 

This knowledge is also used in the orthogonal weight initialization method for Neural Networks.


\end{answer}

\end{question}

%----------------------------------------------

\end{questions}
